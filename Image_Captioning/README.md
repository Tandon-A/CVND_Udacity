# Image Captioning

The goal of this project is to develop a deep learning model to generate captions for images. This is done using a CNN - RNN architecture following the paper [Show and Tell](https://arxiv.org/pdf/1411.4555.pdf). 

![CNN-RNN model](https://raw.githubusercontent.com/Tandon-A/CVND_Udacity/master/Image_Captioning/images/cnn_rnn_model.png)

###### Fig 1: [CNN RNN model](https://arxiv.org/pdf/1411.4555.pdf)

First, the input image is processed by CNN to output image features. An imagenet pretrained Resnet50 is used for the CNN model. The produced image features are then processed by the RNN architecture to generate captions for the input image. 
For the RNN model, a single layer LSTM model has been used. To effectively train the LSTM, the forget gate bias is initialized to 1 as suggested in the [paper](http://proceedings.mlr.press/v37/jozefowicz15.pdf). The model is trained in a supervised manner using [MS COCO dataset](http://cocodataset.org/#home). 

Some sample captions generated by the trained model are shown below. 

![Caption 1](https://raw.githubusercontent.com/Tandon-A/CVND_Udacity/master/Image_Captioning/images/cap1.jpg)
![Caption 2](https://raw.githubusercontent.com/Tandon-A/CVND_Udacity/master/Image_Captioning/images/cap2.jpg)
![Caption 3](https://raw.githubusercontent.com/Tandon-A/CVND_Udacity/master/Image_Captioning/images/cap3.jpg)

###### Fig 2: Generated Image Captions

## Applications

Image captioning can be used to provide verbal descriptions to partially/complete visually impaired people through a headset. It can also be used to build a query based image search engine without the need of manually annotated images. 

## Acknowledgement 

[Udacity Computer Vision Nanodegree](https://github.com/udacity/CVND---Image-Captioning-Project) 

## Author 
[Abhishek Tandon](https://github.com/Tandon-A)
